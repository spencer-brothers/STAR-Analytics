{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Home Credit Default Risk\n",
        "\n",
        "*Can you predict how capable each applicant is of repaying a loan?*\n",
        "\n",
        "Spencer Brothers\n",
        "\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- Set up a training set and a validation set using application_train.csv data set to do cross-validation.  Alternatively you could perform cross-validation using a different framework, such as k-fold cross validation as implemented in modeling packages such as caret or tidymodels or scikit-learn. The model performance that matters, of course, is the estimated performance on the test set as well as the Kaggle score.\n",
        "\n",
        "- Identify the performance benchmark established by the majority class classifier.\n",
        "\n",
        "- Fit several different logistic regression models using different predictors. Do interaction terms improve the model?  Compare model performance using not just accuracy but also AUC.\n",
        "Explore using algorithms like random forest and gradient boosting. Compare model performance.\n",
        "\n",
        "- Perform the data transformations required by a given algorithm.  For example, some algorithms require numeric data and perform better when it has been standardized or normalized.\n",
        "Experiment with upsampling and downsampling the data to adjust for the imbalanced target variable.  (See APM Ch. 16.)  Does this strategy this improve model performance?\n",
        "\n",
        "- Try combining model predictions--this is called an ensemble model--to improve performance.\n",
        "\n",
        "- Try additional feature engineering to boost model performance. Can you combine variables or bin numeric variables?  Explore the notebooks at Kaggle for data transformation ideas. In particular, use the other data sets at Kaggle--beyond the application data--to create additional features.\n",
        "\n",
        "- For machine learning models experiment with hyperparameter tuning  to try to boost performance.\n"
      ],
      "metadata": {
        "id": "SIx18NInsiEI"
      },
      "id": "SIx18NInsiEI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents"
      ],
      "metadata": {
        "id": "DBea059DQh9-"
      },
      "id": "DBea059DQh9-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "The next step is to explore different modeling ideas for the project, with the aim of developing a model that beats a benchmark model (such as the majority class classifier) and produces results that -- hopefully! -- can be used to solve the business problem.\n",
        "\n",
        "### Business Problem\n",
        "\n",
        "Most lending services are based on credit, which excludes a large demographic of people (those with no credit history) from buying a home. Taking an uninformed lending approach is an unsustainable business practice that may leave underserved populations worse off, so using smart lending practices is essential to both Home Credit’s longevity and financial equity for unbanked populations.\n",
        "\n",
        "### Benefit of a Solution\n",
        "\n",
        "By better modeling clients’ behaviors, Home Credit can successfully predict clients’ repayment abilities. This supports Home Credit’s goals in two key areas:\n",
        "\n",
        "1.\tHome Credit will decrease costs of clients defaulting on loans or making late payments, supporting Home Credit's sustainability in an ever-changing economic and political ecosystem.\n",
        "\n",
        "2.\tClients capable of repayment will receive necessary resources that empower their financial success when other financial institutions fail to lend. Loans will be given with principal, maturity, and a repayment schedule that optimizes clients’ lending experience\n",
        "\n",
        "### Objectives of this notebook\n",
        "\n",
        "*TO DO: Rewrite this section when finished with other sections*\n",
        "\n",
        "- Practice feature engineering to improve model performance.\n",
        "\n",
        "- Practice cross-validation.\n",
        "\n",
        "- Learn about the properties of different modeling algorithms by experimenting with different methods and comparing different candidate models.\n",
        "\n",
        "- Learn from group members."
      ],
      "metadata": {
        "id": "h9J4CJkMQpNx"
      },
      "id": "h9J4CJkMQpNx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "Describe any additional data preparation related to modeling:\n",
        "\n",
        "- variable transformations\n",
        "- feature engineering\n",
        "- handling of NAs.\n",
        "\n",
        "### Setup\n",
        "\n",
        "import libraries and read in data"
      ],
      "metadata": {
        "id": "E9iMrtd9Qwgd"
      },
      "id": "E9iMrtd9Qwgd"
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "M6OmJTSaU28g"
      },
      "id": "M6OmJTSaU28g",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "Tl8MbEpi1-_2"
      },
      "id": "Tl8MbEpi1-_2",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount to drive to access data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqwmz8KAODe7",
        "outputId": "e1b5859c-9cb9-4b56-e0fe-69e606de308e"
      },
      "id": "Pqwmz8KAODe7",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read raw data into pandas dataframes\n",
        "data_folder = '/content/drive/MyDrive/MSBA_Practice_Project/data'\n",
        "\n",
        "app_train = pd.read_csv(f'{data_folder}/application_train.csv')\n",
        "app_test = pd.read_csv(f'{data_folder}/application_test.csv')"
      ],
      "metadata": {
        "id": "i0uBao6zYebp"
      },
      "id": "i0uBao6zYebp",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering\n",
        "\n",
        "Along with `application_{train|test}.csv`, Home Credit also includes transactional datasets with information about each applicant. For this project, we will use `previous_application.csv` to hopefully improve our model performance. Because there are potentially multiple records for each applicant in `previous_application.csv`, we will make the following aggregations before joining the transactional data with `application_[train|test}.csv`:\n",
        "\n",
        "1. Proportion of Past Loans Refused (PROP_NAME_PREV_REFUSED)\n",
        "    - A high proportion of past loan refusals may indicate financial instability or a history of risky borrowing behavior, suggesting a higher likelihood of default.\n",
        "3. Average Time Since Loan Decision (AVG_DAYS_DECISION)\n",
        "\t- The recency of past credit decisions can provide insight into financial behavior. Frequent recent loan applications may indicate financial distress or an increased reliance on borrowing.\n",
        "4. Average Previous Credit Amount (AVG_PREV_CREDIT)\n",
        "\t- The typical size of previous loans can serve as an indicator of financial habits. Larger past loans may suggest significant debt obligations, which could impact the ability to repay future loans.\n",
        "5. Average Down Payment (AVG_DOWN_PAYMENT)\n",
        "\t- Consistently low down payments may suggest over-leveraging, increasing the risk of default by indicating a lack of financial reserves.\n",
        "6. Average Repayment Discrepancy (AVG_PREV_REPAYMENT_DISC)\n",
        "\t- The difference between the expected and actual last due date for previous loans can highlight repayment behavior. Large discrepancies may indicate late payments or loan extensions, both of which are potential risk factors.\n",
        "7. Average Rate of Down Payment (AVG_RATE_DOWN_PAYMENT)\n",
        "\t- A lower down payment rate may suggest that borrowers are stretching their finances to secure a loan, which could indicate higher financial risk.\n",
        "8. Count of Previous Loans (CNT_PREV_LOANS)\n",
        "\t- The total number of previous loans provides insight into borrowing patterns. A high number of past loans could indicate experience in managing debt but may also suggest a dependency on credit, which could be a risk factor."
      ],
      "metadata": {
        "id": "L_uJgcwv5zLE"
      },
      "id": "L_uJgcwv5zLE"
    },
    {
      "cell_type": "code",
      "source": [
        "# read transactional data into dataframe\n",
        "prev_app = pd.read_csv(f'{data_folder}/previous_application.csv')\n",
        "\n",
        "# Create a new column for the difference between DAYS_LAST_DUE and DAYS_LAST_DUE_1ST_VERSION\n",
        "prev_app[\"PREV_REPAYMENT_DISC\"] = prev_app[\"DAYS_LAST_DUE\"] - prev_app[\"DAYS_LAST_DUE_1ST_VERSION\"]\n",
        "\n",
        "# Aggregate previous applications\n",
        "prev_agg = prev_app.groupby(\"SK_ID_CURR\").agg(\n",
        "    # Proportion of past loans refused\n",
        "    PROP_NAME_PREV_REFUSED=(\"NAME_CONTRACT_STATUS\", lambda x: (x == \"Refused\").mean()),\n",
        "\n",
        "    # Average time since loan decision (recent approvals may indicate cash flow issues)\n",
        "    AVG_DAYS_DECISION=(\"DAYS_DECISION\", \"mean\"),\n",
        "\n",
        "    # Average previous credit amount (larger past loans might indicate risky borrowing behavior)\n",
        "    AVG_PREV_CREDIT=(\"AMT_CREDIT\", \"mean\"),\n",
        "\n",
        "    # Average down payment (low down payments suggest high leverage, possible risk)\n",
        "    AVG_DOWN_PAYMENT=(\"AMT_DOWN_PAYMENT\", \"mean\"),\n",
        "\n",
        "    # Average difference between actual and expected last due date (delayed repayment = risk)\n",
        "    AVG_PREV_REPAYMENT_DISC=(\"PREV_REPAYMENT_DISC\", \"mean\"),\n",
        "\n",
        "    # Average rate of down payment (low rates could indicate over-leveraging)\n",
        "    AVG_RATE_DOWN_PAYMENT=(\"RATE_DOWN_PAYMENT\", \"mean\"),\n",
        "\n",
        "    # Count of previous loans (a high number of past loans might indicate dependency on credit)\n",
        "    CNT_PREV_LOANS=(\"SK_ID_PREV\", \"count\")\n",
        ").reset_index()\n",
        "\n",
        "# Merge aggregated previous applications with application_train\n",
        "train_merged = app_train.merge(prev_agg, on=\"SK_ID_CURR\", how=\"left\")"
      ],
      "metadata": {
        "id": "NZ9JXUqY7gFI"
      },
      "id": "NZ9JXUqY7gFI",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Data\n",
        "\n",
        "Many columns in the main dataset are mostly comprised of missing data, and most rows are missing data. By using binning and simple median imputation, we can fill all the missing data. While we may lose some detail in our data with binning, it offers a more accurate view of many highly predictive columns that may have structural missingness or where missing data represents a difference in populations, like in EXT_SOURCE{1|2|3}.\n",
        "\n",
        "#### Missing Categorical Data\n",
        "\n",
        "We used an LLM (Chat GPT 4o) to analyze the data dictionary, `HomeCredit_columns_description.csv`, and return a list of categorical features. We use this list to handle missing values in categorical columns, as well as encoding them as integers for model building and evaluation.\n",
        "\n",
        "While many categoriacal variables are nominal, we will use label encoding (assume all categorical variables are ordinal) to prevent excess dimensionality on an already large dataset. To prevent non-linear relationships within categorical columns from impacting model performance, we will focus on training models that handle non-linear relationships well like support vector machines (SVMs) with non-linear kernels, random forests, and gradient boosters"
      ],
      "metadata": {
        "id": "W36_VKiheIor"
      },
      "id": "W36_VKiheIor"
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\n",
        "    'TARGET', 'NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',\n",
        "    'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'ORGANIZATION_TYPE',\n",
        "    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE',\n",
        "    'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'FONDKAPREMONT_MODE',\n",
        "    'HOUSETYPE_MODE', 'TOTALAREA_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE', 'OWN_CAR_AGE', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG','YEARS_BUILD_AVG','COMMONAREA_AVG','ELEVATORS_AVG',\n",
        "    'ENTRANCES_AVG','FLOORSMIN_AVG','LANDAREA_AVG','LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI',\n",
        "    'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMIN_MEDI',  'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'FLAG_DOCUMENT_2',\n",
        "    'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n",
        "    'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'REG_REGION_NOT_LIVE_REGION',\n",
        "    'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY'\n",
        "]\n",
        "\n",
        "numeric_columns = [col for col in app_train.columns.values if col not in categorical_columns]\n",
        "# len(numeric_columns) + len(categorical_columns) # sanity check: should be 122\n",
        "\n",
        "# encode categorical columns\n",
        "for col in tqdm(categorical_columns):\n",
        "  le = LabelEncoder()\n",
        "  app_train[col] = le.fit_transform(app_train[col].astype(str))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8jbhCHbfzXT",
        "outputId": "ae7ca956-2612-48aa-d849-5adfa26c70be"
      },
      "id": "n8jbhCHbfzXT",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [00:15<00:00,  6.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Numeric Data\n",
        "\n"
      ],
      "metadata": {
        "id": "4SEUox8q3872"
      },
      "id": "4SEUox8q3872"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling Process\n",
        "\n",
        "Discuss the candidate models you considered, the process for selecting the best model, cross-validation procedures, hyperparameter tuning."
      ],
      "metadata": {
        "id": "0ngf0mn9Q_Z-"
      },
      "id": "0ngf0mn9Q_Z-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance\n",
        "\n",
        "Describe the performance characteristics of your best model, including run time. What is the train set and test set performance? What is your Kaggle score?"
      ],
      "metadata": {
        "id": "8H0iYupRRR6V"
      },
      "id": "8H0iYupRRR6V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Summarize and discuss your findings."
      ],
      "metadata": {
        "id": "k6NfMhTbRbBc"
      },
      "id": "k6NfMhTbRbBc"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}