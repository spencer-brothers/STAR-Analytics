{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Home Credit Default Risk\n",
        "\n",
        "*Can you predict how capable each applicant is of repaying a loan?*\n",
        "\n",
        "Spencer Brothers\n",
        "\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- Set up a training set and a validation set using application_train.csv data set to do cross-validation.  Alternatively you could perform cross-validation using a different framework, such as k-fold cross validation as implemented in modeling packages such as caret or tidymodels or scikit-learn. The model performance that matters, of course, is the estimated performance on the test set as well as the Kaggle score.\n",
        "\n",
        "- Identify the performance benchmark established by the majority class classifier.\n",
        "\n",
        "- Fit several different logistic regression models using different predictors. Do interaction terms improve the model?  Compare model performance using not just accuracy but also AUC.\n",
        "Explore using algorithms like random forest and gradient boosting. Compare model performance.\n",
        "\n",
        "- Perform the data transformations required by a given algorithm.  For example, some algorithms require numeric data and perform better when it has been standardized or normalized.\n",
        "Experiment with upsampling and downsampling the data to adjust for the imbalanced target variable.  (See APM Ch. 16.)  Does this strategy this improve model performance?\n",
        "\n",
        "- Try combining model predictions--this is called an ensemble model--to improve performance.\n",
        "\n",
        "- Try additional feature engineering to boost model performance. Can you combine variables or bin numeric variables?  Explore the notebooks at Kaggle for data transformation ideas. In particular, use the other data sets at Kaggle--beyond the application data--to create additional features.\n",
        "\n",
        "- For machine learning models experiment with hyperparameter tuning  to try to boost performance.\n"
      ],
      "metadata": {
        "id": "SIx18NInsiEI"
      },
      "id": "SIx18NInsiEI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents"
      ],
      "metadata": {
        "id": "DBea059DQh9-"
      },
      "id": "DBea059DQh9-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "The next step is to explore different modeling ideas for the project, with the aim of developing a model that beats a benchmark model (such as the majority class classifier) and produces results that -- hopefully! -- can be used to solve the business problem.\n",
        "\n",
        "### Business Problem\n",
        "\n",
        "Most lending services are based on credit, which excludes a large demographic of people (those with no credit history) from buying a home. Taking an uninformed lending approach is an unsustainable business practice that may leave underserved populations worse off, so using smart lending practices is essential to both Home Credit’s longevity and financial equity for unbanked populations.\n",
        "\n",
        "### Benefit of a Solution\n",
        "\n",
        "By better understanding clients’ behaviors, Home Credit can successfully predict clients’ repayment abilities. This supports Home Credit’s goals in two key areas:\n",
        "\n",
        "1.\tHome Credit will decrease costs of clients defaulting on loans, supporting Home Credit's sustainability in an ever-changing economic and political ecosystem.\n",
        "\n",
        "2.\tClients capable of repayment will receive necessary resources that empower their financial success when other financial institutions fail to lend. Loans will be given with principal, maturity, and a repayment schedule that optimizes clients’ lending experience\n",
        "\n",
        "### Objectives of this notebook\n",
        "\n",
        "*TO DO: Rewrite this section when finished with other sections*\n",
        "\n",
        "- Practice feature engineering to improve model performance.\n",
        "\n",
        "- Practice cross-validation.\n",
        "\n",
        "- Learn about the properties of different modeling algorithms by experimenting with different methods and comparing different candidate models.\n",
        "\n",
        "- Learn from group members."
      ],
      "metadata": {
        "id": "h9J4CJkMQpNx"
      },
      "id": "h9J4CJkMQpNx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "Describe any additional data preparation related to modeling:\n",
        "\n",
        "- variable transformations\n",
        "- feature engineering\n",
        "- handling of NAs.\n",
        "\n",
        "### Setup"
      ],
      "metadata": {
        "id": "E9iMrtd9Qwgd"
      },
      "id": "E9iMrtd9Qwgd"
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "M6OmJTSaU28g"
      },
      "id": "M6OmJTSaU28g",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount to drive to access data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Pqwmz8KAODe7",
        "outputId": "e1b5859c-9cb9-4b56-e0fe-69e606de308e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Pqwmz8KAODe7",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read raw data into pandas dataframes\n",
        "data_folder = '/content/drive/MyDrive/MSBA_Practice_Project/data'\n",
        "\n",
        "app_train = pd.read_csv(f'{data_folder}/application_train.csv')\n",
        "app_test = pd.read_csv(f'{data_folder}/application_test.csv')"
      ],
      "metadata": {
        "id": "i0uBao6zYebp"
      },
      "id": "i0uBao6zYebp",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Data\n",
        "\n",
        "Many columns in the main dataset are mostly comprised of missing data, and most rows are missing data. By using binning and simple median imputation, we can fill all the missing data. While we may lose some detail in our data with binning, it offers a more accurate view of many highly predictive columns that may have structural missingness or where missing data represents a difference in populations, like in EXT_SOURCE{1|2|3}.\n",
        "\n",
        "#### Missing Categorical Data\n",
        "\n",
        "We used an LLM (Chat GPT 4o) to analyze the data dictionary, `HomeCredit_columns_description.csv`, and return a list of categorical features. We use this list to handle missing values in categorical columns, as well as converting them to strings. They will later be converted to integers for model building and evaluation."
      ],
      "metadata": {
        "id": "W36_VKiheIor"
      },
      "id": "W36_VKiheIor"
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\n",
        "    'TARGET', 'NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',\n",
        "    'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'ORGANIZATION_TYPE',\n",
        "    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE',\n",
        "    'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'FONDKAPREMONT_MODE',\n",
        "    'HOUSETYPE_MODE', 'TOTALAREA_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE', 'OWN_CAR_AGE', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG','YEARS_BUILD_AVG','COMMONAREA_AVG','ELEVATORS_AVG',\n",
        "    'ENTRANCES_AVG','FLOORSMIN_AVG','LANDAREA_AVG','LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI',\n",
        "    'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMIN_MEDI',  'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'FLAG_DOCUMENT_2',\n",
        "    'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n",
        "    'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'REG_REGION_NOT_LIVE_REGION',\n",
        "    'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY'\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "n8jbhCHbfzXT"
      },
      "id": "n8jbhCHbfzXT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling Process\n",
        "\n",
        "Discuss the candidate models you considered, the process for selecting the best model, cross-validation procedures, hyperparameter tuning."
      ],
      "metadata": {
        "id": "0ngf0mn9Q_Z-"
      },
      "id": "0ngf0mn9Q_Z-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance\n",
        "\n",
        "Describe the performance characteristics of your best model, including run time. What is the train set and test set performance? What is your Kaggle score?"
      ],
      "metadata": {
        "id": "8H0iYupRRR6V"
      },
      "id": "8H0iYupRRR6V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Summarize and discuss your findings."
      ],
      "metadata": {
        "id": "k6NfMhTbRbBc"
      },
      "id": "k6NfMhTbRbBc"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}